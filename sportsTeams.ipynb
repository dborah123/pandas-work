{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pandas Sports Team Exercise\n",
    "This is a project I did when I taught myself pandas through Coursera's Intro to Data Science with Python course. The first 4 cells focus on the 4 big North American sports leagues. In each cell, I calculate the win loss per city. For instance, the win loss of NFL teams in LA will be different than just the win loss of teh Chargers as this also factors in the win loss of the Rams. The last cell can be used to compute the p-values of the correlations found."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'assets/nhl.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7e8fc411e4db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnhl_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"assets/nhl.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mcities\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"assets/wikipedia_data.html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcities\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets/nhl.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "# nhl_df=pd.read_csv(\"assets/nhl.csv\")\n",
    "# cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "# cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "def nhl_correlation(): \n",
    "    #Retrieving and downloading data\n",
    "    nhl_df=pd.read_csv(\"assets/nhl.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "\n",
    "    #Setting columns/rows\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "\n",
    "    #Cleaning cities names and dropping useless data\n",
    "    cities['NHL'].replace('(\\[[\\w ]+\\])', '', regex = True, inplace = True)\n",
    "    cities['NHL'].replace('(\\—)', np.nan, regex = True, inplace = True)\n",
    "    cities['NHL'].replace('', np.nan, regex = True, inplace = True)\n",
    "    cities['NHL'].dropna()\n",
    "\n",
    "    #Sorting cities and fitlering out useless ones\n",
    "    cities = cities.sort_values('Metropolitan area')\n",
    "    cities.index = cities['Metropolitan area']\n",
    "    cities = cities[['NHL', 'Population (2016 est.)[8]']].dropna()\n",
    "    \n",
    "    #Calculating win loss in NHL DF\n",
    "    nhl_df['team'].replace(\"(\\*)\", '', regex = True, inplace = True)\n",
    "    nhl_df.index = nhl_df['team']\n",
    "    nhl_df = nhl_df[nhl_df['year'] == 2018]\n",
    "\n",
    "    #Dropping team titles that are actually division titles\n",
    "    nhl_df.drop('Atlantic Division', axis = 0, inplace = True)\n",
    "    nhl_df.drop('Pacific Division', axis = 0, inplace = True)\n",
    "    nhl_df.drop('Metropolitan Division', axis = 0, inplace = True)\n",
    "    nhl_df.drop('Central Division', axis = 0, inplace = True)\n",
    "    \n",
    "    #Sorting index\n",
    "    nhl_df.sort_index(ascending = True, inplace = True)\n",
    "\n",
    "    #Setting cities to teams\n",
    "    nhl_df['Metropolitan area'] = pd.Series(['Los Angeles', 'Phoenix', 'Boston', 'Buffalo', 'Calgary', 'Raleigh', 'Chicago',\n",
    "    'Denver', 'Columbus', 'Dallas–Fort Worth', 'Detroit', 'Edmonton', 'Miami–Fort Lauderdale', 'Los Angeles', 'Minneapolis–Saint Paul',\n",
    "    'Montreal', 'Nashville', 'New York City', 'New York City', 'New York City', 'Ottawa', 'Philadelphia', 'Pittsburgh', 'San Francisco Bay Area',\n",
    "    'St. Louis', 'Tampa Bay Area', 'Toronto', 'Vancouver', 'Las Vegas', 'Washington, D.C.', 'Winnipeg'], index = nhl_df.index)\n",
    "\n",
    "    #Calculating Win Loss\n",
    "    nhl_df['W'] = nhl_df['W'].apply(lambda x: int(x))\n",
    "    nhl_df['L'] = nhl_df['L'].apply(lambda x: int(x))\n",
    "    nhl_df['winloss'] = nhl_df['W']/(nhl_df['W'] + nhl_df['L'])\n",
    "    nhl_df1 = nhl_df.groupby('Metropolitan area').agg('mean')\n",
    "    \n",
    "    population_by_region = cities['Population (2016 est.)[8]'].dropna().astype(int) # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = nhl_df1['winloss'] # pass in win/loss ratio from nhl_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    #Return correlation between pop and win loss by city\n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nba_correlation():\n",
    "    #Retrieiving and reading data\n",
    "    nba_df=pd.read_csv(\"assets/nba.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "    \n",
    "    #Cleaning Cities:\n",
    "    cities['NBA'].replace('(\\[[\\w ]+\\])', '', regex = True, inplace = True)\n",
    "    cities['NBA'].replace('(\\—)', np.nan, regex = True, inplace = True)\n",
    "    cities['NBA'].replace('', np.nan, regex = True, inplace = True)\n",
    "\n",
    "    #Sorting and setting index\n",
    "    cities = cities.sort_values('Metropolitan area')\n",
    "    cities.index = cities['Metropolitan area']\n",
    "    cities = cities.dropna()\n",
    "    \n",
    "    #Cleaning nba_df and sorting data\n",
    "    nba_df['team'].replace('(\\*\\s\\([0-9]*\\)$)', '', regex = True, inplace = True)\n",
    "    nba_df['team'].replace('(\\([0-9]*\\))','', regex = True, inplace = True)\n",
    "    nba_df = nba_df[nba_df['year'] == 2018]\n",
    "    nba_df.sort_values('team', inplace = True)\n",
    "    ndb_df = nba_df.set_index('team')\n",
    "\n",
    "    #Assigning cities to teams\n",
    "    ct = ['Atlanta', 'Boston', 'New York City', 'Charlotte', 'Chicago', 'Cleveland', 'Dallas–Fort Worth', \n",
    "                               'Denver', 'Detroit', 'San Francisco Bay Area','Houston', 'Indianapolis', 'Los Angeles', 'Los Angeles',\n",
    "                                'Memphis', 'Miami–Fort Lauderdale', 'Milwaukee', 'Minneapolis–Saint Paul', 'New Orleans', \n",
    "                                'New York City', 'Oklahoma City', 'Orlando', 'Philadelphia', 'Phoenix', 'Portland', 'Sacramento', \n",
    "                                'San Antonio', 'Toronto', 'Salt Lake City', 'Washington, D.C.']\n",
    "\n",
    "    \n",
    "    #Calculating win loss\n",
    "    nba_df['City'] = pd.Series(ct, index = nba_df.index)\n",
    "    nba_df['W/L%'] = nba_df['W/L%'].apply(lambda x: float(x))\n",
    "\n",
    "    nba_df = nba_df.groupby('City').agg('mean')\n",
    "    \n",
    "    \n",
    "    population_by_region = cities['Population (2016 est.)[8]'].astype(int) # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = nba_df['W/L%'] # pass in win/loss ratio from nba_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlb_correlation(): \n",
    "    #Retreiving data and reading it\n",
    "    mlb_df=pd.read_csv(\"assets/mlb.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "    #Cleaning team names and corting\n",
    "    cities['MLB'].replace('(\\[[\\w ]+\\])', '', regex = True, inplace = True)\n",
    "    cities['MLB'].replace('(\\—)', np.nan, regex = True, inplace = True)\n",
    "    cities['MLB'].replace('', np.nan, regex = True, inplace = True)\n",
    "    cities = cities.sort_values('Metropolitan area')\n",
    "    cities.index = cities['Metropolitan area']\n",
    "    cities = cities.dropna()\n",
    "    \n",
    "    #Cleaing and seperating data for mlb_df:\n",
    "    mlb_df = mlb_df[mlb_df['year'] == 2018]\n",
    "    mlb_df.sort_values('team', inplace = True)\n",
    "    mlb_df = mlb_df.set_index('team')\n",
    "\n",
    "    #Assigning cites to teams\n",
    "    ct = ['Phoenix', 'Atlanta', 'Baltimore', 'Boston', 'Chicago', 'Chicago', 'Cincinatti', 'Cleveland', 'Denver', 'Detroit', \n",
    "         'Houston', 'Kansas City', 'Los Angeles', 'Los Angeles', 'Miami–Fort Lauderdale', 'Milwaukee', 'Minneapolis–Saint Paul',\n",
    "         'New York City', 'New York City', 'San Francisco Bay Area', 'Philadephia', 'Pittsburgh', 'San Diego', \n",
    "         'San Francisco Bay Area', 'Seattle', 'St. Louis', 'Tampa Bay Area', 'Dallas–Fort Worth', 'Toronto', 'Washington, D.C.']\n",
    "\n",
    "    #Calculating win loss\n",
    "    mlb_df['City'] = pd.Series(ct, index = mlb_df.index)\n",
    "    mlb_df['W-L%'] = mlb_df['W-L%'].apply(lambda x: float(x))\n",
    "    mlb_df = mlb_df.groupby('City').agg('mean')\n",
    "    \n",
    "    population_by_region = cities['Population (2016 est.)[8]'].astype(int) # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = mlb_df['W-L%'] # pass in win/loss ratio from mlb_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfl_correlation(): \n",
    "    #Getting and reading data\n",
    "    nfl_df=pd.read_csv(\"assets/nfl.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "    #Cleaning city names\n",
    "    cities['NFL'].replace('(\\[[\\w ]+\\])', '', regex = True, inplace = True)\n",
    "    cities['NFL'].replace('(\\—)', np.nan, regex = True, inplace = True)\n",
    "    cities['NFL'].replace('', np.nan, regex = True, inplace = True)\n",
    "    cities = cities.sort_values('Metropolitan area')\n",
    "    cities.index = cities['Metropolitan area']\n",
    "    cities = cities.dropna()\n",
    "    \n",
    "    #Cleaning and seperating nfl_df data:\n",
    "    nfl_df = nfl_df[nfl_df['year'] == 2018]\n",
    "    nfl_df.sort_values('team', inplace = True)\n",
    "    nfl_df['team'].replace('([*+]+)', '', regex = True, inplace = True)\n",
    "\n",
    "    nfl_df = nfl_df.set_index('team')\n",
    "\n",
    "    nfl_df = nfl_df[4:]\n",
    "\n",
    "    #Setting cities to team\n",
    "    ct = ['Phoenix', 'Atlanta', 'Baltimore', 'Buffalo', 'Charlotte', 'Chicago', 'Cincinnati', 'Cleveland', 'Dallas–Fort Worth',\n",
    "         'Denver', 'Detroit', 'Green Bay', 'Houston', 'Indianapolis', 'Jacksonville', 'Kansas City', 'Los Angeles', 'Los Angeles', \n",
    "         'Miami–Fort Lauderdale', 'Minneapolis–Saint Paul', 'Boston', 'New Orleans', 'New York City', 'New York City', \n",
    "         'San Francisco Bay Area', 'Philadeplhia', 'Pittsburgh', 'San Francisco Bay Area', 'Seattle', 'Tampa Bay Area', 'Nashville', \n",
    "         'Washington, D.C.']\n",
    "\n",
    "    #Cleaning and calculating win loss\n",
    "    nfl_df = nfl_df.drop(['NFC East', 'NFC North', 'NFC West', 'NFC South'])\n",
    "    nfl_df['City'] = pd.Series(ct, index = nfl_df.index)\n",
    "    nfl_df['W-L%'] = nfl_df['W-L%'].apply(lambda x: float(x))\n",
    "    nfl_df = nfl_df.groupby('City').agg('mean')\n",
    "    \n",
    "    population_by_region = cities['Population (2016 est.)[8]'].astype(int) # pass in metropolitan area population from cities\n",
    "    win_loss_by_region = nfl_df['W-L%'] # pass in win/loss ratio from nfl_df in the same order as cities[\"Metropolitan area\"]\n",
    "\n",
    "    return stats.pearsonr(population_by_region, win_loss_by_region)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sports_team_performance():\n",
    "    '''\n",
    "    Calculating and returns p values\n",
    "    '''\n",
    "    mlb_df=pd.read_csv(\"assets/mlb.csv\")\n",
    "    nhl_df=pd.read_csv(\"assets/nhl.csv\")\n",
    "    nba_df=pd.read_csv(\"assets/nba.csv\")\n",
    "    nfl_df=pd.read_csv(\"assets/nfl.csv\")\n",
    "    cities=pd.read_html(\"assets/wikipedia_data.html\")[1]\n",
    "    cities=cities.iloc[:-1,[0,3,5,6,7,8]]\n",
    "\n",
    "    sports = ['NFL', 'NBA', 'NHL', 'MLB']\n",
    "    p_values = pd.DataFrame({k:np.nan for k in sports}, index=sports)\n",
    "    \n",
    "    return p_values"
   ]
  }
 ]
}